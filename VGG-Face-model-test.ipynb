{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "e7a16fae-e0f8-4ab1-89e3-457c6bcabe63"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'During the training of the deep networks, we oversample the training\\nimages by rotating them around their center by a random angle between \\n\\xe2\\x88\\x9215\\xc2\\xb0 and 15\\xc2\\xb0, and by circularly shifting the images in the horizontal and \\nvertical directions by an amount no more than 20% of the image size. \\nThis approach helps our network to be more robust against alignment errors. \\nIn Fig. 2, we show the training curves of two stages of fine-tuning of the \\nnetwork with the FER dataset, where we set the learning rate and weight \\ndecay to 0.0005, momentum to 0.9, and dropout probability to 0.8 [25].'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "For the fine-tuning of the VGG-Face network for the emotion\n",
    "recognition task, we investigated various options in our preliminary\n",
    "analysis. We found that combining weight decay and dropout for regularization\n",
    "gives the best results on the FER validation set. We carry\n",
    "out a multi-stage fine-tuning. In the first stage, we fine-tune on the\n",
    "FER public test set, and run weight updates for five epochs. In the second\n",
    "stage, we update the upper layers (higher than layer 27) using'''\n",
    "\n",
    "''' We then fine-tune the VGG-face model on FER 2013\n",
    "dataset, using both the training and the public test set; during\n",
    "training we use data augmentation by jittering the scale, flipping\n",
    "and rotating the faces. The aim is to make the network more robust\n",
    "to small misalignment of the faces. We also apply a strong dropout\n",
    "on the last layer of the VGG (keeping only 5% of the nodes) to\n",
    "prevent over-fitting. We achieve a performance of 71.2% on the\n",
    "FER private test set, which is slightly higher than the previously\n",
    "published results '''\n",
    "\n",
    "'''During the training of the deep networks, we oversample the training\n",
    "images by rotating them around their center by a random angle between \n",
    "−15° and 15°, and by circularly shifting the images in the horizontal and \n",
    "vertical directions by an amount no more than 20% of the image size. \n",
    "This approach helps our network to be more robust against alignment errors. \n",
    "In Fig. 2, we show the training curves of two stages of fine-tuning of the \n",
    "network with the FER dataset, where we set the learning rate and weight \n",
    "decay to 0.0005, momentum to 0.9, and dropout probability to 0.8 [25].'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6879088e-ce15-45e6-8f39-29fd16ab529a"
    }
   },
   "source": [
    "### VGG16-Face model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "d691c9bc-5b8d-42ae-b109-0597f42f5d60"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import VGG_FACE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "570025bf-5112-4fae-9432-4039578fc868"
    }
   },
   "outputs": [],
   "source": [
    "model = VGG_FACE.VGG_FACE\n",
    "\n",
    "model.load_state_dict(torch.load('VGG_FACE.pth'))\n",
    "\n",
    "#model.eval() #this will let you pass an input through the model and evaluate it? without training?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "7ef650af-4012-42c1-a5f5-69e04cdd9cf8"
    }
   },
   "source": [
    "### Dataset: FERplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "5e29d967-b841-48d2-8160-e678660be2ea"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from skimage import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "be8542c0-7e28-4b37-93f3-0fda5361a126"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Usage</th>\n",
       "      <th>Image name</th>\n",
       "      <th>neutral</th>\n",
       "      <th>happiness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>sadness</th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>contempt</th>\n",
       "      <th>unknown</th>\n",
       "      <th>NF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Training</td>\n",
       "      <td>fer0000000.png</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Training</td>\n",
       "      <td>fer0000001.png</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Training</td>\n",
       "      <td>fer0000002.png</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Training</td>\n",
       "      <td>fer0000003.png</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Training</td>\n",
       "      <td>fer0000004.png</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Training</td>\n",
       "      <td>fer0000005.png</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Training</td>\n",
       "      <td>fer0000006.png</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Training</td>\n",
       "      <td>fer0000007.png</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Training</td>\n",
       "      <td>fer0000008.png</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Training</td>\n",
       "      <td>fer0000009.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Training</td>\n",
       "      <td>fer0000010.png</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Training</td>\n",
       "      <td>fer0000011.png</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Training</td>\n",
       "      <td>fer0000012.png</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Training</td>\n",
       "      <td>fer0000013.png</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Training</td>\n",
       "      <td>fer0000014.png</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Training</td>\n",
       "      <td>fer0000015.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Training</td>\n",
       "      <td>fer0000016.png</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Training</td>\n",
       "      <td>fer0000018.png</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Training</td>\n",
       "      <td>fer0000019.png</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Training</td>\n",
       "      <td>fer0000020.png</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Training</td>\n",
       "      <td>fer0000021.png</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Training</td>\n",
       "      <td>fer0000022.png</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Training</td>\n",
       "      <td>fer0000024.png</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Training</td>\n",
       "      <td>fer0000025.png</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Training</td>\n",
       "      <td>fer0000026.png</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Training</td>\n",
       "      <td>fer0000027.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Training</td>\n",
       "      <td>fer0000028.png</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Training</td>\n",
       "      <td>fer0000029.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Training</td>\n",
       "      <td>fer0000030.png</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Training</td>\n",
       "      <td>fer0000031.png</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35856</th>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>fer0035771.png</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35857</th>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>fer0035772.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35858</th>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>fer0035773.png</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35859</th>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>fer0035774.png</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35860</th>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>fer0035775.png</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35861</th>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>fer0035776.png</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35862</th>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>fer0035777.png</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35863</th>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>fer0035778.png</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35864</th>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>fer0035779.png</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35865</th>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>fer0035780.png</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35866</th>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>fer0035781.png</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35867</th>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>fer0035782.png</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35868</th>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>fer0035783.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35869</th>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>fer0035784.png</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35870</th>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>fer0035785.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35871</th>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>fer0035786.png</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35872</th>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>fer0035787.png</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35873</th>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>fer0035788.png</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35874</th>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>fer0035789.png</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35875</th>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>fer0035790.png</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35876</th>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>fer0035791.png</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35877</th>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>fer0035792.png</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35878</th>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>fer0035793.png</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35879</th>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>fer0035794.png</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35880</th>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>fer0035795.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35881</th>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>fer0035796.png</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35882</th>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>fer0035797.png</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35884</th>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>fer0035799.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35885</th>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>fer0035800.png</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35886</th>\n",
       "      <td>PrivateTest</td>\n",
       "      <td>fer0035801.png</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35714 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Usage      Image name  neutral  happiness  surprise  sadness  \\\n",
       "0         Training  fer0000000.png        4          0         0        1   \n",
       "1         Training  fer0000001.png        6          0         1        1   \n",
       "2         Training  fer0000002.png        5          0         0        3   \n",
       "3         Training  fer0000003.png        4          0         0        4   \n",
       "4         Training  fer0000004.png        9          0         0        1   \n",
       "5         Training  fer0000005.png        6          0         0        1   \n",
       "6         Training  fer0000006.png        2          0         0        8   \n",
       "7         Training  fer0000007.png        0         10         0        0   \n",
       "8         Training  fer0000008.png        0         10         0        0   \n",
       "9         Training  fer0000009.png        0          0         6        0   \n",
       "10        Training  fer0000010.png        2          0         0        0   \n",
       "11        Training  fer0000011.png       10          0         0        0   \n",
       "12        Training  fer0000012.png        5          0         0        3   \n",
       "13        Training  fer0000013.png        9          0         0        1   \n",
       "14        Training  fer0000014.png        0         10         0        0   \n",
       "15        Training  fer0000015.png        0          0         6        0   \n",
       "16        Training  fer0000016.png        4          6         0        0   \n",
       "18        Training  fer0000018.png        1          0         2        4   \n",
       "19        Training  fer0000019.png        6          1         0        0   \n",
       "20        Training  fer0000020.png        5          0         0        4   \n",
       "21        Training  fer0000021.png        1          0         1        2   \n",
       "22        Training  fer0000022.png        1          0         1        1   \n",
       "24        Training  fer0000024.png        2          7         0        0   \n",
       "25        Training  fer0000025.png        0         10         0        0   \n",
       "26        Training  fer0000026.png        4          2         4        0   \n",
       "27        Training  fer0000027.png        0          0         0        0   \n",
       "28        Training  fer0000028.png        1          7         0        0   \n",
       "29        Training  fer0000029.png        0          1         6        0   \n",
       "30        Training  fer0000030.png        0          9         0        0   \n",
       "31        Training  fer0000031.png       10          0         0        0   \n",
       "...            ...             ...      ...        ...       ...      ...   \n",
       "35856  PrivateTest  fer0035771.png        0         10         0        0   \n",
       "35857  PrivateTest  fer0035772.png        0          0        10        0   \n",
       "35858  PrivateTest  fer0035773.png        1          0         1        7   \n",
       "35859  PrivateTest  fer0035774.png       10          0         0        0   \n",
       "35860  PrivateTest  fer0035775.png        0         10         0        0   \n",
       "35861  PrivateTest  fer0035776.png        9          0         0        0   \n",
       "35862  PrivateTest  fer0035777.png        2          0         3        0   \n",
       "35863  PrivateTest  fer0035778.png        0          2         7        0   \n",
       "35864  PrivateTest  fer0035779.png        4          1         0        4   \n",
       "35865  PrivateTest  fer0035780.png        0         10         0        0   \n",
       "35866  PrivateTest  fer0035781.png        7          3         0        0   \n",
       "35867  PrivateTest  fer0035782.png        0         10         0        0   \n",
       "35868  PrivateTest  fer0035783.png        0          0         0        0   \n",
       "35869  PrivateTest  fer0035784.png        0         10         0        0   \n",
       "35870  PrivateTest  fer0035785.png        0          0         0        0   \n",
       "35871  PrivateTest  fer0035786.png        9          1         0        0   \n",
       "35872  PrivateTest  fer0035787.png        6          0         0        0   \n",
       "35873  PrivateTest  fer0035788.png        8          0         0        2   \n",
       "35874  PrivateTest  fer0035789.png        1          0         1        5   \n",
       "35875  PrivateTest  fer0035790.png        2          0         5        1   \n",
       "35876  PrivateTest  fer0035791.png        1          9         0        0   \n",
       "35877  PrivateTest  fer0035792.png        4          0         0        5   \n",
       "35878  PrivateTest  fer0035793.png        0         10         0        0   \n",
       "35879  PrivateTest  fer0035794.png        3          0         0        5   \n",
       "35880  PrivateTest  fer0035795.png        0          0         1        6   \n",
       "35881  PrivateTest  fer0035796.png        5          0         0        3   \n",
       "35882  PrivateTest  fer0035797.png        8          0         0        2   \n",
       "35884  PrivateTest  fer0035799.png        0          0         0        0   \n",
       "35885  PrivateTest  fer0035800.png        0         10         0        0   \n",
       "35886  PrivateTest  fer0035801.png        2          0         0        5   \n",
       "\n",
       "       anger  disgust  fear  contempt  unknown  NF  \n",
       "0          3        2     0         0        0   0  \n",
       "1          0        0     0         0        2   0  \n",
       "2          1        0     0         0        1   0  \n",
       "3          1        0     0         0        1   0  \n",
       "4          0        0     0         0        0   0  \n",
       "5          0        0     1         1        1   0  \n",
       "6          0        0     0         0        0   0  \n",
       "7          0        0     0         0        0   0  \n",
       "8          0        0     0         0        0   0  \n",
       "9          0        0     4         0        0   0  \n",
       "10         8        0     0         0        0   0  \n",
       "11         0        0     0         0        0   0  \n",
       "12         0        0     0         0        2   0  \n",
       "13         0        0     0         0        0   0  \n",
       "14         0        0     0         0        0   0  \n",
       "15         1        0     3         0        0   0  \n",
       "16         0        0     0         0        0   0  \n",
       "18         2        0     0         0        1   0  \n",
       "19         3        0     0         0        0   0  \n",
       "20         0        0     0         0        1   0  \n",
       "21         0        0     5         0        1   0  \n",
       "22         7        0     0         0        0   0  \n",
       "24         0        0     0         1        0   0  \n",
       "25         0        0     0         0        0   0  \n",
       "26         0        0     0         0        0   0  \n",
       "27         7        1     0         0        2   0  \n",
       "28         0        2     0         0        0   0  \n",
       "29         0        0     3         0        0   0  \n",
       "30         0        1     0         0        0   0  \n",
       "31         0        0     0         0        0   0  \n",
       "...      ...      ...   ...       ...      ...  ..  \n",
       "35856      0        0     0         0        0   0  \n",
       "35857      0        0     0         0        0   0  \n",
       "35858      0        0     0         0        1   0  \n",
       "35859      0        0     0         0        0   0  \n",
       "35860      0        0     0         0        0   0  \n",
       "35861      0        0     0         0        1   0  \n",
       "35862      0        0     5         0        0   0  \n",
       "35863      0        0     1         0        0   0  \n",
       "35864      0        0     0         0        1   0  \n",
       "35865      0        0     0         0        0   0  \n",
       "35866      0        0     0         0        0   0  \n",
       "35867      0        0     0         0        0   0  \n",
       "35868      7        0     2         0        1   0  \n",
       "35869      0        0     0         0        0   0  \n",
       "35870     10        0     0         0        0   0  \n",
       "35871      0        0     0         0        0   0  \n",
       "35872      1        1     1         0        1   0  \n",
       "35873      0        0     0         0        0   0  \n",
       "35874      0        0     1         0        2   0  \n",
       "35875      0        0     0         1        1   0  \n",
       "35876      0        0     0         0        0   0  \n",
       "35877      0        0     0         0        1   0  \n",
       "35878      0        0     0         0        0   0  \n",
       "35879      1        0     0         0        1   0  \n",
       "35880      0        0     3         0        0   0  \n",
       "35881      0        0     0         0        2   0  \n",
       "35882      0        0     0         0        0   0  \n",
       "35884      7        1     0         2        0   0  \n",
       "35885      0        0     0         0        0   0  \n",
       "35886      1        1     0         0        1   0  \n",
       "\n",
       "[35714 rows x 12 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('all-data/FERPlus/fer2013new.csv')\n",
    "df.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "0ca1abe4-7bb7-4bbe-8faf-080cad04eaf5"
    }
   },
   "outputs": [],
   "source": [
    "class FaceEmotionsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.emotions_frame = pd.read_csv(csv_file)\n",
    "        self.emotions_frame.dropna(axis=0, how='any', inplace=True)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.emotions_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.emotions_frame.iloc[idx][1]\n",
    "        \n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        image = io.imread(img_path)\n",
    "        #this takes the most highest ranked emotion. if two emotions have the same ranking, it just takes the first one\n",
    "        emotion = np.argmax(self.emotions_frame.iloc[idx,2:].as_matrix())\n",
    "        if (emotion == 1 or emotion == 2):\n",
    "            emotion = 2\n",
    "        elif (emotion > 2 and emotion < 8):\n",
    "            emotion = 0\n",
    "        else:\n",
    "            emotion = 1\n",
    "        \n",
    "        sample = {'image': image, 'emotion': emotion}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "nbpresent": {
     "id": "252d81d5-707f-4133-add3-fe798d2dc52e"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'int'>\n"
     ]
    }
   ],
   "source": [
    "face_emotions = FaceEmotionsDataset(csv_file = 'all-data/FERPlus/fer2013new_training.csv', \n",
    "                                    root_dir = 'all-data/FERPlus/data/FER2013Train')\n",
    "\n",
    "print(type(face_emotions[0]['emotion']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "nbpresent": {
     "id": "0d088434-f0c7-4d61-a29f-29d2529e3ad1"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from skimage import transform\n",
    "from PIL import Image\n",
    "from skimage import io; io.use_plugin('matplotlib')\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, emotion = sample['image'], sample['emotion']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        #print(image.shape)\n",
    "        #image = image.transpose((2, 0, 1))\n",
    "        image = np.expand_dims(image,0)\n",
    "        y = np.copy(image)\n",
    "        z = np.copy(image)\n",
    "        z = np.concatenate((y,z), axis=0)    \n",
    "        image = np.concatenate((image,z), axis=0)\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'emotion': emotion}\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, emotion = sample['image'], sample['emotion']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "\n",
    "        image = image[top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "\n",
    "        #emotion = emotion - [left, top]\n",
    "\n",
    "        return {'image': image, 'emotion': emotion}\n",
    "\n",
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or tuple): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, emotion = sample['image'], sample['emotion']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "\n",
    "        # h and w are swapped for landmarks because for images,\n",
    "        # x and y axes are axis 1 and 0 respectively\n",
    "        \n",
    "        #emotion = emotion * [new_w / w, new_h / h]\n",
    "\n",
    "        return {'image': img, 'emotion': emotion}\n",
    "\n",
    "\n",
    "imgTransform = transforms.Compose([Rescale(256), #scale to 256x256\n",
    "                                   #transforms.CenterCrop(224), #crops the image at center to 224x224\n",
    "                                   RandomCrop(224),\n",
    "                                   ToTensor()\n",
    "                                   ])\n",
    "                                   #, #turn the jpg/pil/wahtever image into a tensor\n",
    "                                   #transforms.Normalize(mean = [0.485, 0.456, 0.406], #normalize with these vals\n",
    "                                                        #std=[0.229, 0.224, 0.225])])\n",
    "                                    ##HOW TO GET NORMALIZED VALUES?\n",
    "                                    #to add: jitter/rotate data augmentation, flipping, \n",
    "                                   \n",
    "#this doesn't work because the data is organized w a csv file w prob distrib of labels \n",
    "#instead of a single ground truth\n",
    "#see this paper: https://arxiv.org/pdf/1608.01041.pdf\n",
    "#trainset = FaceEmotionsDataset(csv_file = 'all-data/FERPlus/fer2013new_training.csv', \n",
    "#                                    root_dir = 'all-data/FERPlus/data/FER2013Train', transform = imgTransform)\n",
    "#valset = FaceEmotionsDataset(csv_file = 'all-data/FERPlus/fer2013new_validation.csv', \n",
    "#                                    root_dir = 'all-data/FERPlus/data/FER2013Valid', transform = imgTransform)\n",
    "\n",
    "trainset = FaceEmotionsDataset(csv_file = 'all-data/FERPlus/fer2013new_short.csv', \n",
    "                                    root_dir = 'all-data/FERPlus/data/FER2013Train', transform = imgTransform)\n",
    "valset = FaceEmotionsDataset(csv_file = 'all-data/FERPlus/fer2013new_short.csv', \n",
    "                                    root_dir = 'all-data/FERPlus/data/FER2013Train', transform = imgTransform)\n",
    "\n",
    "trainLoader = torch.utils.data.DataLoader(trainset, batch_size = 9, \n",
    "                                          shuffle = True, num_workers = 0)\n",
    "valLoader = torch.utils.data.DataLoader(valset, batch_size = 9, \n",
    "                                       shuffle = False, num_workers = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "( 0 , 0 ,.,.) = \n",
      "  0.3076  0.3059  0.3042  ...   0.2806  0.2833  0.2860\n",
      "  0.3060  0.3038  0.3018  ...   0.2837  0.2858  0.2879\n",
      "  0.3063  0.3039  0.3019  ...   0.2849  0.2867  0.2884\n",
      "           ...             ⋱             ...          \n",
      "  0.2916  0.2942  0.2950  ...   0.6102  0.5712  0.5321\n",
      "  0.2954  0.2967  0.2962  ...   0.6347  0.6049  0.5750\n",
      "  0.2991  0.2992  0.2975  ...   0.6592  0.6386  0.6180\n",
      "\n",
      "( 0 , 1 ,.,.) = \n",
      "  0.3076  0.3059  0.3042  ...   0.2806  0.2833  0.2860\n",
      "  0.3060  0.3038  0.3018  ...   0.2837  0.2858  0.2879\n",
      "  0.3063  0.3039  0.3019  ...   0.2849  0.2867  0.2884\n",
      "           ...             ⋱             ...          \n",
      "  0.2916  0.2942  0.2950  ...   0.6102  0.5712  0.5321\n",
      "  0.2954  0.2967  0.2962  ...   0.6347  0.6049  0.5750\n",
      "  0.2991  0.2992  0.2975  ...   0.6592  0.6386  0.6180\n",
      "\n",
      "( 0 , 2 ,.,.) = \n",
      "  0.3076  0.3059  0.3042  ...   0.2806  0.2833  0.2860\n",
      "  0.3060  0.3038  0.3018  ...   0.2837  0.2858  0.2879\n",
      "  0.3063  0.3039  0.3019  ...   0.2849  0.2867  0.2884\n",
      "           ...             ⋱             ...          \n",
      "  0.2916  0.2942  0.2950  ...   0.6102  0.5712  0.5321\n",
      "  0.2954  0.2967  0.2962  ...   0.6347  0.6049  0.5750\n",
      "  0.2991  0.2992  0.2975  ...   0.6592  0.6386  0.6180\n",
      "      ⋮  \n",
      "\n",
      "( 1 , 0 ,.,.) = \n",
      "  0.0000  0.0000  0.0000  ...   0.0668  0.0808  0.0929\n",
      "  0.0000  0.0000  0.0000  ...   0.0617  0.0749  0.0865\n",
      "  0.0000  0.0000  0.0000  ...   0.0566  0.0690  0.0801\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.1635  0.1672  0.1699\n",
      "  0.0000  0.0000  0.0000  ...   0.1627  0.1664  0.1691\n",
      "  0.0000  0.0000  0.0000  ...   0.1620  0.1657  0.1684\n",
      "\n",
      "( 1 , 1 ,.,.) = \n",
      "  0.0000  0.0000  0.0000  ...   0.0668  0.0808  0.0929\n",
      "  0.0000  0.0000  0.0000  ...   0.0617  0.0749  0.0865\n",
      "  0.0000  0.0000  0.0000  ...   0.0566  0.0690  0.0801\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.1635  0.1672  0.1699\n",
      "  0.0000  0.0000  0.0000  ...   0.1627  0.1664  0.1691\n",
      "  0.0000  0.0000  0.0000  ...   0.1620  0.1657  0.1684\n",
      "\n",
      "( 1 , 2 ,.,.) = \n",
      "  0.0000  0.0000  0.0000  ...   0.0668  0.0808  0.0929\n",
      "  0.0000  0.0000  0.0000  ...   0.0617  0.0749  0.0865\n",
      "  0.0000  0.0000  0.0000  ...   0.0566  0.0690  0.0801\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.1635  0.1672  0.1699\n",
      "  0.0000  0.0000  0.0000  ...   0.1627  0.1664  0.1691\n",
      "  0.0000  0.0000  0.0000  ...   0.1620  0.1657  0.1684\n",
      "      ⋮  \n",
      "\n",
      "( 2 , 0 ,.,.) = \n",
      "  0.6518  0.6440  0.6363  ...   0.2083  0.1995  0.1908\n",
      "  0.6525  0.6446  0.6367  ...   0.2081  0.1979  0.1877\n",
      "  0.6532  0.6452  0.6372  ...   0.2080  0.1963  0.1845\n",
      "           ...             ⋱             ...          \n",
      "  0.9799  0.9814  0.9829  ...   0.3020  0.2946  0.2872\n",
      "  0.9818  0.9831  0.9843  ...   0.3123  0.3052  0.2981\n",
      "  0.9832  0.9842  0.9852  ...   0.3233  0.3165  0.3096\n",
      "\n",
      "( 2 , 1 ,.,.) = \n",
      "  0.6518  0.6440  0.6363  ...   0.2083  0.1995  0.1908\n",
      "  0.6525  0.6446  0.6367  ...   0.2081  0.1979  0.1877\n",
      "  0.6532  0.6452  0.6372  ...   0.2080  0.1963  0.1845\n",
      "           ...             ⋱             ...          \n",
      "  0.9799  0.9814  0.9829  ...   0.3020  0.2946  0.2872\n",
      "  0.9818  0.9831  0.9843  ...   0.3123  0.3052  0.2981\n",
      "  0.9832  0.9842  0.9852  ...   0.3233  0.3165  0.3096\n",
      "\n",
      "( 2 , 2 ,.,.) = \n",
      "  0.6518  0.6440  0.6363  ...   0.2083  0.1995  0.1908\n",
      "  0.6525  0.6446  0.6367  ...   0.2081  0.1979  0.1877\n",
      "  0.6532  0.6452  0.6372  ...   0.2080  0.1963  0.1845\n",
      "           ...             ⋱             ...          \n",
      "  0.9799  0.9814  0.9829  ...   0.3020  0.2946  0.2872\n",
      "  0.9818  0.9831  0.9843  ...   0.3123  0.3052  0.2981\n",
      "  0.9832  0.9842  0.9852  ...   0.3233  0.3165  0.3096\n",
      "...     \n",
      "      ⋮  \n",
      "\n",
      "( 6 , 0 ,.,.) = \n",
      "  0.2157  0.2157  0.2157  ...   0.4664  0.4686  0.4709\n",
      "  0.2157  0.2157  0.2157  ...   0.4726  0.4751  0.4776\n",
      "  0.2157  0.2157  0.2157  ...   0.4788  0.4816  0.4844\n",
      "           ...             ⋱             ...          \n",
      "  0.2235  0.2235  0.2223  ...   0.2330  0.2225  0.2120\n",
      "  0.2234  0.2235  0.2224  ...   0.2213  0.2120  0.2028\n",
      "  0.2232  0.2235  0.2225  ...   0.2095  0.2015  0.1935\n",
      "\n",
      "( 6 , 1 ,.,.) = \n",
      "  0.2157  0.2157  0.2157  ...   0.4664  0.4686  0.4709\n",
      "  0.2157  0.2157  0.2157  ...   0.4726  0.4751  0.4776\n",
      "  0.2157  0.2157  0.2157  ...   0.4788  0.4816  0.4844\n",
      "           ...             ⋱             ...          \n",
      "  0.2235  0.2235  0.2223  ...   0.2330  0.2225  0.2120\n",
      "  0.2234  0.2235  0.2224  ...   0.2213  0.2120  0.2028\n",
      "  0.2232  0.2235  0.2225  ...   0.2095  0.2015  0.1935\n",
      "\n",
      "( 6 , 2 ,.,.) = \n",
      "  0.2157  0.2157  0.2157  ...   0.4664  0.4686  0.4709\n",
      "  0.2157  0.2157  0.2157  ...   0.4726  0.4751  0.4776\n",
      "  0.2157  0.2157  0.2157  ...   0.4788  0.4816  0.4844\n",
      "           ...             ⋱             ...          \n",
      "  0.2235  0.2235  0.2223  ...   0.2330  0.2225  0.2120\n",
      "  0.2234  0.2235  0.2224  ...   0.2213  0.2120  0.2028\n",
      "  0.2232  0.2235  0.2225  ...   0.2095  0.2015  0.1935\n",
      "      ⋮  \n",
      "\n",
      "( 7 , 0 ,.,.) = \n",
      "  0.1418  0.1509  0.1600  ...   0.1875  0.1942  0.2009\n",
      "  0.1534  0.1632  0.1730  ...   0.1799  0.1847  0.1894\n",
      "  0.1650  0.1755  0.1860  ...   0.1724  0.1752  0.1780\n",
      "           ...             ⋱             ...          \n",
      "  0.2294  0.2189  0.2084  ...   0.3829  0.3794  0.3759\n",
      "  0.2537  0.2434  0.2330  ...   0.3979  0.3987  0.3995\n",
      "  0.2730  0.2637  0.2543  ...   0.4047  0.4081  0.4116\n",
      "\n",
      "( 7 , 1 ,.,.) = \n",
      "  0.1418  0.1509  0.1600  ...   0.1875  0.1942  0.2009\n",
      "  0.1534  0.1632  0.1730  ...   0.1799  0.1847  0.1894\n",
      "  0.1650  0.1755  0.1860  ...   0.1724  0.1752  0.1780\n",
      "           ...             ⋱             ...          \n",
      "  0.2294  0.2189  0.2084  ...   0.3829  0.3794  0.3759\n",
      "  0.2537  0.2434  0.2330  ...   0.3979  0.3987  0.3995\n",
      "  0.2730  0.2637  0.2543  ...   0.4047  0.4081  0.4116\n",
      "\n",
      "( 7 , 2 ,.,.) = \n",
      "  0.1418  0.1509  0.1600  ...   0.1875  0.1942  0.2009\n",
      "  0.1534  0.1632  0.1730  ...   0.1799  0.1847  0.1894\n",
      "  0.1650  0.1755  0.1860  ...   0.1724  0.1752  0.1780\n",
      "           ...             ⋱             ...          \n",
      "  0.2294  0.2189  0.2084  ...   0.3829  0.3794  0.3759\n",
      "  0.2537  0.2434  0.2330  ...   0.3979  0.3987  0.3995\n",
      "  0.2730  0.2637  0.2543  ...   0.4047  0.4081  0.4116\n",
      "      ⋮  \n",
      "\n",
      "( 8 , 0 ,.,.) = \n",
      "  0.3495  0.3486  0.3473  ...   0.3475  0.3301  0.3126\n",
      "  0.4598  0.4587  0.4570  ...   0.4573  0.4343  0.4113\n",
      "  0.5702  0.5688  0.5666  ...   0.5670  0.5385  0.5100\n",
      "           ...             ⋱             ...          \n",
      "  0.2345  0.2249  0.2486  ...   0.6615  0.6698  0.6781\n",
      "  0.2384  0.2372  0.2710  ...   0.6588  0.6685  0.6782\n",
      "  0.2423  0.2496  0.2935  ...   0.6562  0.6673  0.6784\n",
      "\n",
      "( 8 , 1 ,.,.) = \n",
      "  0.3495  0.3486  0.3473  ...   0.3475  0.3301  0.3126\n",
      "  0.4598  0.4587  0.4570  ...   0.4573  0.4343  0.4113\n",
      "  0.5702  0.5688  0.5666  ...   0.5670  0.5385  0.5100\n",
      "           ...             ⋱             ...          \n",
      "  0.2345  0.2249  0.2486  ...   0.6615  0.6698  0.6781\n",
      "  0.2384  0.2372  0.2710  ...   0.6588  0.6685  0.6782\n",
      "  0.2423  0.2496  0.2935  ...   0.6562  0.6673  0.6784\n",
      "\n",
      "( 8 , 2 ,.,.) = \n",
      "  0.3495  0.3486  0.3473  ...   0.3475  0.3301  0.3126\n",
      "  0.4598  0.4587  0.4570  ...   0.4573  0.4343  0.4113\n",
      "  0.5702  0.5688  0.5666  ...   0.5670  0.5385  0.5100\n",
      "           ...             ⋱             ...          \n",
      "  0.2345  0.2249  0.2486  ...   0.6615  0.6698  0.6781\n",
      "  0.2384  0.2372  0.2710  ...   0.6588  0.6685  0.6782\n",
      "  0.2423  0.2496  0.2935  ...   0.6562  0.6673  0.6784\n",
      "[torch.DoubleTensor of size 9x3x224x224]\n",
      ", \n",
      " 2\n",
      " 1\n",
      " 1\n",
      " 0\n",
      " 1\n",
      " 2\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.LongTensor of size 9]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (i, sample) in enumerate(trainLoader):\n",
    "    inputs = sample['image']\n",
    "    labels = sample['emotion']\n",
    "    print(\"{}, {}\".format(inputs, labels))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6d2ad753-43bf-4df1-b903-4dc8b543cff0"
    }
   },
   "source": [
    "### Function to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "d0d19064-5fe3-4fae-b953-b62bf3aeba34"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c460d37436be4c018935189b8e44d715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('label is', Variable containing:\n",
      " 1\n",
      "[torch.LongTensor of size 1]\n",
      ")\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nClvZylaW5P8Dn+6wEGlZ1lwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f65ef911f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "t = tqdm(trainLoader)\n",
    "for (i, (sample)) in enumerate(t):\n",
    "    \n",
    "    inputs = Variable(sample['image'] ).float()#.cpu()\n",
    "    labels = Variable(sample['emotion'])\n",
    "    #model = modified_pretrained.cpu()\n",
    "    #outputs = modified_pretrained(inputs)\n",
    "    #print(outputs)\n",
    "    #print(inputs)\n",
    "    print(\"label is\", labels[i])\n",
    "    showme = (inputs.data.numpy()[i]).transpose(1,2,0)\n",
    "    plt.imshow(showme)\n",
    "    plt.show(showme.all)\n",
    "    \n",
    "    if i == 20:\n",
    "        break\n",
    "    #outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "c180c3a4-a698-4501-a48d-3b4cb152b2f8"
    }
   },
   "outputs": [],
   "source": [
    "import lab_utils\n",
    "def train_model(network, criterion, optimizer, trainLoader, valLoader, n_epochs = 10, use_gpu = False):\n",
    "    if use_gpu:\n",
    "        network = network.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "        \n",
    "    t_loss, t_acc, v_loss, v_acc = (np.zeros(n_epochs) for i in range(4))    \n",
    "    \n",
    "    # Training loop.\n",
    "    for epoch in range(0, n_epochs):\n",
    "        correct = 0.0\n",
    "        cum_loss = 0.0\n",
    "        counter = 0\n",
    "\n",
    "        # Make a pass over the training data.\n",
    "        t = tqdm(trainLoader, desc = 'Training epoch %d' % epoch)\n",
    "        network.train()  # This is important to call before training!\n",
    "        for (i, (sample)) in enumerate(t):\n",
    "            # Wrap inputs, and targets into torch.autograd.Variable types.\n",
    "            inputs = Variable(sample['image']).float()\n",
    "            print(\"inputs\", inputs[0])\n",
    "            labels = Variable(sample['emotion'])\n",
    "            print(\"requires grad?\", inputs.requires_grad)\n",
    "            #print(\"training\", sample['emotion'])\n",
    "            if use_gpu:\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            # Forward pass:\n",
    "            outputs = network(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            print(\"outputs\", outputs.data)\n",
    "            print(\"labels\", labels.data)\n",
    "\n",
    "            # Backward pass:\n",
    "            optimizer.zero_grad()\n",
    "            # Loss is a variable, and calling backward on a Variable will\n",
    "            # compute all the gradients that lead to that Variable taking on its\n",
    "            # current value.\n",
    "            #a = list(network.parameters())[0].clone()\n",
    "            print(\"loss\",loss.data)\n",
    "            loss.backward() \n",
    "            \n",
    "            for param in network.parameters():\n",
    "                print(\"gradient values:\", param.grad.data.sum())\n",
    "                break\n",
    "            \n",
    "            # Weight and bias updates.\n",
    "            optimizer.step()\n",
    "            b = list(network.parameters())[0].clone()\n",
    "            #print(torch.equal(a.data, b.data))\n",
    "\n",
    "            # logging information.\n",
    "            cum_loss += loss.data[0]\n",
    "            max_scores, max_labels = outputs.data.max(1)\n",
    "            correct += (max_labels == labels.data).sum()\n",
    "            counter += inputs.size(0)\n",
    "            t.set_postfix(loss = cum_loss / (1 + i), accuracy = 100 * correct / counter)\n",
    "            \n",
    "        t_loss[epoch] = (cum_loss/len(t))\n",
    "        t_acc[epoch] = (100*correct/counter)\n",
    "\n",
    "        # Make a pass over the validation data.\n",
    "        correct = 0.0\n",
    "        cum_loss = 0.0\n",
    "        counter = 0\n",
    "        t = tqdm(valLoader, desc = 'Validation epoch %d' % epoch)\n",
    "        network.eval()  # This is important to call before evaluating!\n",
    "        for (i, (sample)) in enumerate(t):\n",
    "            #print(\"on iter {}\".format(i))\n",
    "            # Wrap inputs, and targets into torch.autograd.Variable types.\n",
    "            inputs = Variable(sample['image'] ).float()\n",
    "            labels = Variable(sample['emotion'])\n",
    "            #print(\"validation\", sample['emotion'])\n",
    "            if use_gpu:\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "            \n",
    "            # Forward pass:\n",
    "            outputs = network(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            \n",
    "            # logging information.\n",
    "            cum_loss += loss.data[0]\n",
    "            max_scores, max_labels = outputs.data.max(1)\n",
    "            correct += (max_labels == labels.data).sum()\n",
    "            counter += inputs.size(0)\n",
    "            t.set_postfix(loss = cum_loss / (1 + i), accuracy = 100 * correct / counter)\n",
    "            \n",
    "        v_loss[epoch] = (cum_loss/len(t))\n",
    "        v_acc[epoch] = (100*correct/counter)\n",
    "        \n",
    "                \n",
    "    lab_utils.generate_plots(t_loss, v_loss, t_acc, v_acc, n_epochs)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "8459ae87-2706-43ee-9348-a63be5996153"
    }
   },
   "source": [
    "### set learning rate, loss, optimizer, all variable stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential (\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU ()\n",
      "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU ()\n",
      "  (4): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (6): ReLU ()\n",
      "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): ReLU ()\n",
      "  (9): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU ()\n",
      "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (13): ReLU ()\n",
      "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): ReLU ()\n",
      "  (16): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (18): ReLU ()\n",
      "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (20): ReLU ()\n",
      "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (22): ReLU ()\n",
      "  (23): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (25): ReLU ()\n",
      "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (27): ReLU ()\n",
      "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (29): ReLU ()\n",
      "  (30): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "  (31): Lambda (\n",
      "  )\n",
      "  (32): Sequential (\n",
      "    (0): Lambda (\n",
      "    )\n",
      "    (1): Linear (25088 -> 4096)\n",
      "  )\n",
      "  (33): ReLU ()\n",
      "  (34): Dropout (p = 0.5)\n",
      "  (35): Sequential (\n",
      "    (0): Lambda (\n",
      "    )\n",
      "    (1): Linear (4096 -> 4096)\n",
      "  )\n",
      "  (36): ReLU ()\n",
      "  (37): Dropout (p = 0.5)\n",
      "  (38): Sequential (\n",
      "    (0): Lambda (\n",
      "    )\n",
      "    (1): Linear (4096 -> 3)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = model\n",
    "#print(pretrained_model)\n",
    "#print(list(pretrained_model.children())[:-2]) #gets rid of the last linear layer and softmax\n",
    "modified_pretrained = nn.Sequential(*list(pretrained_model.children())[:-2]) \n",
    "\n",
    "#print(modified_pretrained)\n",
    "\n",
    "modified_pretrained.add_module('38', nn.Sequential(VGG_FACE.Lambda(lambda x: x.view(1,-1) if 1==len(x.size()) else x ),nn.Linear(4096,3)))\n",
    "#modified_pretrained.add_module('39', nn.Softmax())\n",
    "\n",
    "print(modified_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "nbpresent": {
     "id": "faa6cc19-fa86-49d0-9374-cfc76c652add"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bc77a2ecca240b898fafb7a928a0430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'Training epoch 0: ', max=1), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('inputs', Variable containing:\n",
      "( 0 ,.,.) = \n",
      "  0.1177  0.1224  0.1271  ...   0.6985  0.6942  0.6902\n",
      "  0.1154  0.1207  0.1259  ...   0.6990  0.6948  0.6909\n",
      "  0.1131  0.1189  0.1247  ...   0.6995  0.6954  0.6916\n",
      "           ...             ⋱             ...          \n",
      "  0.4261  0.4500  0.4738  ...   0.6592  0.6577  0.6561\n",
      "  0.4129  0.4343  0.4557  ...   0.6570  0.6555  0.6539\n",
      "  0.3997  0.4186  0.4375  ...   0.6548  0.6533  0.6517\n",
      "\n",
      "( 1 ,.,.) = \n",
      "  0.1177  0.1224  0.1271  ...   0.6985  0.6942  0.6902\n",
      "  0.1154  0.1207  0.1259  ...   0.6990  0.6948  0.6909\n",
      "  0.1131  0.1189  0.1247  ...   0.6995  0.6954  0.6916\n",
      "           ...             ⋱             ...          \n",
      "  0.4261  0.4500  0.4738  ...   0.6592  0.6577  0.6561\n",
      "  0.4129  0.4343  0.4557  ...   0.6570  0.6555  0.6539\n",
      "  0.3997  0.4186  0.4375  ...   0.6548  0.6533  0.6517\n",
      "\n",
      "( 2 ,.,.) = \n",
      "  0.1177  0.1224  0.1271  ...   0.6985  0.6942  0.6902\n",
      "  0.1154  0.1207  0.1259  ...   0.6990  0.6948  0.6909\n",
      "  0.1131  0.1189  0.1247  ...   0.6995  0.6954  0.6916\n",
      "           ...             ⋱             ...          \n",
      "  0.4261  0.4500  0.4738  ...   0.6592  0.6577  0.6561\n",
      "  0.4129  0.4343  0.4557  ...   0.6570  0.6555  0.6539\n",
      "  0.3997  0.4186  0.4375  ...   0.6548  0.6533  0.6517\n",
      "[torch.FloatTensor of size 3x224x224]\n",
      ")\n",
      "('requires grad?', True)\n",
      "('outputs', \n",
      "1.00000e-03 *\n",
      "  0.8548  4.0879  1.2022\n",
      "  0.8548  4.0879  1.2022\n",
      "  0.8548  4.0879  1.2022\n",
      "  0.8548  4.0879  1.2022\n",
      "  0.8548  4.0879  1.2022\n",
      "  0.8548  4.0879  1.2022\n",
      "  0.8548  4.0879  1.2022\n",
      "  0.8548  4.0879  1.2022\n",
      "  0.8548  4.0879  1.2022\n",
      "[torch.cuda.FloatTensor of size 9x3 (GPU 0)]\n",
      ")\n",
      "('labels', \n",
      " 1\n",
      " 1\n",
      " 0\n",
      " 2\n",
      " 2\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.cuda.LongTensor of size 9 (GPU 0)]\n",
      ")\n",
      "('loss', \n",
      " 1.0976\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      ")\n",
      "('gradient values:', 0.0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5132122c7d24c4f84fc83d295bd2310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'Validation epoch 0: ', max=1), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e77bef514d54e949ac1fbd22bf6fe21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'Training epoch 1: ', max=1), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('inputs', Variable containing:\n",
      "( 0 ,.,.) = \n",
      "  0.0030  0.0023  0.0017  ...   0.0411  0.0383  0.0354\n",
      "  0.0024  0.0018  0.0013  ...   0.0401  0.0374  0.0346\n",
      "  0.0018  0.0014  0.0010  ...   0.0391  0.0365  0.0339\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.1418  0.1396  0.1374\n",
      "  0.0000  0.0000  0.0000  ...   0.1403  0.1381  0.1359\n",
      "  0.0000  0.0000  0.0000  ...   0.1388  0.1366  0.1344\n",
      "\n",
      "( 1 ,.,.) = \n",
      "  0.0030  0.0023  0.0017  ...   0.0411  0.0383  0.0354\n",
      "  0.0024  0.0018  0.0013  ...   0.0401  0.0374  0.0346\n",
      "  0.0018  0.0014  0.0010  ...   0.0391  0.0365  0.0339\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.1418  0.1396  0.1374\n",
      "  0.0000  0.0000  0.0000  ...   0.1403  0.1381  0.1359\n",
      "  0.0000  0.0000  0.0000  ...   0.1388  0.1366  0.1344\n",
      "\n",
      "( 2 ,.,.) = \n",
      "  0.0030  0.0023  0.0017  ...   0.0411  0.0383  0.0354\n",
      "  0.0024  0.0018  0.0013  ...   0.0401  0.0374  0.0346\n",
      "  0.0018  0.0014  0.0010  ...   0.0391  0.0365  0.0339\n",
      "           ...             ⋱             ...          \n",
      "  0.0000  0.0000  0.0000  ...   0.1418  0.1396  0.1374\n",
      "  0.0000  0.0000  0.0000  ...   0.1403  0.1381  0.1359\n",
      "  0.0000  0.0000  0.0000  ...   0.1388  0.1366  0.1344\n",
      "[torch.FloatTensor of size 3x224x224]\n",
      ")\n",
      "('requires grad?', True)\n",
      "('outputs', \n",
      "1.00000e-03 *\n",
      "  0.6441  4.4039  1.0969\n",
      "  0.6441  4.4039  1.0969\n",
      "  0.6441  4.4039  1.0969\n",
      "  0.6441  4.4039  1.0969\n",
      "  0.6441  4.4039  1.0969\n",
      "  0.6441  4.4039  1.0969\n",
      "  0.6441  4.4039  1.0969\n",
      "  0.6441  4.4039  1.0969\n",
      "  0.6441  4.4039  1.0969\n",
      "[torch.cuda.FloatTensor of size 9x3 (GPU 0)]\n",
      ")\n",
      "('labels', \n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 0\n",
      " 2\n",
      " 1\n",
      " 1\n",
      " 2\n",
      "[torch.cuda.LongTensor of size 9 (GPU 0)]\n",
      ")\n",
      "('loss', \n",
      " 1.0974\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      ")\n",
      "('gradient values:', 0.0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2438c9853ca49788bcfc6f469b0c36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'Validation epoch 1: ', max=1), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2916c2eb83345dda15dcb542d28159c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'Training epoch 2: ', max=1), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('inputs', Variable containing:\n",
      "( 0 ,.,.) = \n",
      "  0.2023  0.2088  0.2152  ...   0.2010  0.1943  0.1876\n",
      "  0.1977  0.2060  0.2140  ...   0.1953  0.1880  0.1808\n",
      "  0.2023  0.2117  0.2202  ...   0.1890  0.1825  0.1759\n",
      "           ...             ⋱             ...          \n",
      "  0.1660  0.1634  0.1607  ...   0.2523  0.2429  0.2335\n",
      "  0.1649  0.1645  0.1634  ...   0.2808  0.2681  0.2554\n",
      "  0.1638  0.1656  0.1661  ...   0.3093  0.2933  0.2773\n",
      "\n",
      "( 1 ,.,.) = \n",
      "  0.2023  0.2088  0.2152  ...   0.2010  0.1943  0.1876\n",
      "  0.1977  0.2060  0.2140  ...   0.1953  0.1880  0.1808\n",
      "  0.2023  0.2117  0.2202  ...   0.1890  0.1825  0.1759\n",
      "           ...             ⋱             ...          \n",
      "  0.1660  0.1634  0.1607  ...   0.2523  0.2429  0.2335\n",
      "  0.1649  0.1645  0.1634  ...   0.2808  0.2681  0.2554\n",
      "  0.1638  0.1656  0.1661  ...   0.3093  0.2933  0.2773\n",
      "\n",
      "( 2 ,.,.) = \n",
      "  0.2023  0.2088  0.2152  ...   0.2010  0.1943  0.1876\n",
      "  0.1977  0.2060  0.2140  ...   0.1953  0.1880  0.1808\n",
      "  0.2023  0.2117  0.2202  ...   0.1890  0.1825  0.1759\n",
      "           ...             ⋱             ...          \n",
      "  0.1660  0.1634  0.1607  ...   0.2523  0.2429  0.2335\n",
      "  0.1649  0.1645  0.1634  ...   0.2808  0.2681  0.2554\n",
      "  0.1638  0.1656  0.1661  ...   0.3093  0.2933  0.2773\n",
      "[torch.FloatTensor of size 3x224x224]\n",
      ")\n",
      "('requires grad?', True)\n",
      "('outputs', \n",
      "1.00000e-03 *\n",
      "  0.3436  4.8546  0.9467\n",
      "  0.3436  4.8546  0.9467\n",
      "  0.3436  4.8546  0.9467\n",
      "  0.3436  4.8546  0.9467\n",
      "  0.3436  4.8546  0.9467\n",
      "  0.3436  4.8546  0.9467\n",
      "  0.3436  4.8546  0.9467\n",
      "  0.3436  4.8546  0.9467\n",
      "  0.3436  4.8546  0.9467\n",
      "[torch.cuda.FloatTensor of size 9x3 (GPU 0)]\n",
      ")\n",
      "('labels', \n",
      " 1\n",
      " 2\n",
      " 1\n",
      " 1\n",
      " 2\n",
      " 0\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.cuda.LongTensor of size 9 (GPU 0)]\n",
      ")\n",
      "('loss', \n",
      " 1.0972\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      ")\n",
      "('gradient values:', 0.0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b3b3fdbb27461692ab4711512cfe32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'Validation epoch 2: ', max=1), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e72b103e6147bf92b6ee3d91da8c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description=u'Training epoch 3: ', max=1), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('inputs', Variable containing:\n",
      "( 0 ,.,.) = \n",
      "  0.2758  0.2747  0.2735  ...   0.1944  0.1879  0.1768\n",
      "  0.2737  0.2716  0.2695  ...   0.1959  0.1894  0.1783\n",
      "  0.2605  0.2591  0.2577  ...   0.1927  0.1872  0.1769\n",
      "           ...             ⋱             ...          \n",
      "  0.2287  0.2272  0.2257  ...   0.3244  0.3391  0.3404\n",
      "  0.2228  0.2213  0.2199  ...   0.3035  0.3148  0.3140\n",
      "  0.2169  0.2154  0.2140  ...   0.2827  0.2904  0.2877\n",
      "\n",
      "( 1 ,.,.) = \n",
      "  0.2758  0.2747  0.2735  ...   0.1944  0.1879  0.1768\n",
      "  0.2737  0.2716  0.2695  ...   0.1959  0.1894  0.1783\n",
      "  0.2605  0.2591  0.2577  ...   0.1927  0.1872  0.1769\n",
      "           ...             ⋱             ...          \n",
      "  0.2287  0.2272  0.2257  ...   0.3244  0.3391  0.3404\n",
      "  0.2228  0.2213  0.2199  ...   0.3035  0.3148  0.3140\n",
      "  0.2169  0.2154  0.2140  ...   0.2827  0.2904  0.2877\n",
      "\n",
      "( 2 ,.,.) = \n",
      "  0.2758  0.2747  0.2735  ...   0.1944  0.1879  0.1768\n",
      "  0.2737  0.2716  0.2695  ...   0.1959  0.1894  0.1783\n",
      "  0.2605  0.2591  0.2577  ...   0.1927  0.1872  0.1769\n",
      "           ...             ⋱             ...          \n",
      "  0.2287  0.2272  0.2257  ...   0.3244  0.3391  0.3404\n",
      "  0.2228  0.2213  0.2199  ...   0.3035  0.3148  0.3140\n",
      "  0.2169  0.2154  0.2140  ...   0.2827  0.2904  0.2877\n",
      "[torch.FloatTensor of size 3x224x224]\n",
      ")\n",
      "('requires grad?', True)\n",
      "('outputs', \n",
      "1.00000e-03 *\n",
      " -0.0377  5.4263  0.7563\n",
      " -0.0377  5.4263  0.7563\n",
      " -0.0377  5.4263  0.7563\n",
      " -0.0377  5.4263  0.7563\n",
      " -0.0377  5.4263  0.7563\n",
      " -0.0377  5.4263  0.7563\n",
      " -0.0377  5.4263  0.7563\n",
      " -0.0377  5.4263  0.7563\n",
      " -0.0377  5.4263  0.7563\n",
      "[torch.cuda.FloatTensor of size 9x3 (GPU 0)]\n",
      ")\n",
      "('labels', \n",
      " 1\n",
      " 2\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 2\n",
      " 0\n",
      " 1\n",
      "[torch.cuda.LongTensor of size 9 (GPU 0)]\n",
      ")\n",
      "('loss', \n",
      " 1.0969\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      ")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-288-544c84cf5300>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# following this model: https://arxiv.org/pdf/1711.04598.pdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodified_pretrained\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-286-bd2393f360cd>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(network, criterion, optimizer, trainLoader, valLoader, n_epochs, use_gpu)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m#a = list(network.parameters())[0].clone()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/crystaljgong/miniconda3/envs/ipykernel_py2/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/crystaljgong/miniconda3/envs/ipykernel_py2/lib/python2.7/site-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#\"where we set the learning rate and weight decay to 0.0005, momentum to 0.9, and dropout probability to 0.8 [25]\"\n",
    "learningRate = 5e-4\n",
    "weightDecay = 0.0005\n",
    "moment = 0.9\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Definition of our network. #how to change the last fc layer of the model to nn.linear(4096, 7) instead of (4096, 2622)?\n",
    "#model.fc = nn.Linear(512, 2)\n",
    "\n",
    "#Definition of our loss. #maybe need to change this?\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Definition of optimization strategy. # maybe need to change this?\n",
    "optimizer = optim.SGD(modified_pretrained.parameters(), lr = learningRate, nesterov = True, weight_decay = weightDecay, momentum= moment)\n",
    "# following this model: https://arxiv.org/pdf/1711.04598.pdf\n",
    "\n",
    "train_model(modified_pretrained, criterion, optimizer, trainLoader, valLoader, n_epochs = 10, use_gpu = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#torch.save(modified_pretrained, 'trained_model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
